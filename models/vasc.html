<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.2" />
<title>dimdrop.models.vasc API documentation</title>
<meta name="description" content="Code adapted from https://github.com/wang-research/VASC" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}#index .two-column{column-count:2}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>dimdrop.models.vasc</code> module</h1>
</header>
<section id="section-intro">
<p>Code adapted from https://github.com/wang-research/VASC</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Code adapted from https://github.com/wang-research/VASC
&#34;&#34;&#34;
# FIXME if possible reimplement annealing for gumbel approximation in more idiomatic keras code

# -*- coding: utf-8 -*-
import h5py
from keras import metrics
from keras.optimizers import RMSprop, Adagrad, Adam
import numpy as np
from keras.utils.layer_utils import print_summary
from keras import regularizers
from keras.models import Model
import keras.backend as K
from keras.layers.merge import concatenate, multiply
from keras.layers import Input, Dense, Activation, Lambda, RepeatVector, Reshape, Layer, Dropout, BatchNormalization, Permute
from keras.callbacks import EarlyStopping
from ..losses import VAELoss


class VASC:
    &#34;&#34;&#34;
    VASC: variational autoencoder for scRNA-seq datasets
    &#34;&#34;&#34;

    def __init__(
            self,
            in_dim,
            out_dim,
            epochs=1000,
            patience=3,
            batch_size=100,
            lr=0.01,
            var=False,
            log=True,
            scale=True,
            verbose=0):
        &#34;&#34;&#34;
        Parameters:
        -----------
        in_dim : int
            The input dimension
        out_dim : int
            The output dimension
        epochs: int, optional
            Maximum number of epochs, default `5000`
        patience: int, optional
            The amount of epochs without improvement before the network stops training, default `3`
        batch_size: int, optional
            The batch size for stochastic optimization, default `100`
        lr : float, optional
            The learning rate of the network, default `0.01`
        var: boolean, optional
            Whether to estimate the variance parameters, default `False`
        log: boolean, optional
            Whether log-transformation should be performed, default `True`
        scale: boolean, optional
            Whether scaling (making values within [0,1]) should be performed, default `True`
        &#34;&#34;&#34;
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.epochs = epochs
        self.patience = patience
        self.batch_size = batch_size
        self.lr = lr
        self.var = var
        self.log = log
        self.scale = scale
        self.verbose = verbose
        self.__init_network()

    def __init_network(self):
        in_dim = self.in_dim
        expr_in = Input(shape=(self.in_dim,))

        # The first part of model to recover the expr.
        h0 = Dropout(0.5)(expr_in)
        # Encoder layers
        h1 = Dense(units=512, name=&#39;encoder_1&#39;,
                   kernel_regularizer=regularizers.l1(0.01))(h0)
        h2 = Dense(units=128, name=&#39;encoder_2&#39;)(h1)
        h2_relu = Activation(&#39;relu&#39;)(h2)
        h3 = Dense(units=32, name=&#39;encoder_3&#39;)(h2_relu)
        h3_relu = Activation(&#39;relu&#39;)(h3)

        z_mean = Dense(units=self.out_dim, name=&#39;z_mean&#39;)(h3_relu)
        z_log_var = None
        if self.var:
            z_log_var = Dense(units=2, name=&#39;z_log_var&#39;)(h3_relu)
            z_log_var = Activation(&#39;softplus&#39;)(z_log_var)

        # sampling new samples
            z = Lambda(sampling, output_shape=(
                self.out_dim,))([z_mean, z_log_var])
        else:
            z = Lambda(sampling, output_shape=(self.out_dim,))([z_mean])

        # Decoder layers
        decoder_h1 = Dense(units=32, name=&#39;decoder_1&#39;)(z)
        decoder_h1_relu = Activation(&#39;relu&#39;)(decoder_h1)
        decoder_h2 = Dense(units=128, name=&#39;decoder_2&#39;)(decoder_h1_relu)
        decoder_h2_relu = Activation(&#39;relu&#39;)(decoder_h2)
        decoder_h3 = Dense(units=512, name=&#39;decoder_3&#39;)(decoder_h2_relu)
        decoder_h3_relu = Activation(&#39;relu&#39;)(decoder_h3)
        expr_x = Dense(units=self.in_dim, activation=&#39;sigmoid&#39;)(
            decoder_h3_relu)

        expr_x_drop = Lambda(lambda x: -x ** 2)(expr_x)
        expr_x_drop_p = Lambda(lambda x: K.exp(x))(expr_x_drop)
        expr_x_nondrop_p = Lambda(lambda x: 1-x)(expr_x_drop_p)
        expr_x_nondrop_log = Lambda(lambda x: K.log(x+1e-20))(expr_x_nondrop_p)
        expr_x_drop_log = Lambda(lambda x: K.log(x+1e-20))(expr_x_drop_p)
        expr_x_drop_log = Reshape(target_shape=(
            self.in_dim, 1))(expr_x_drop_log)
        expr_x_nondrop_log = Reshape(target_shape=(
            self.in_dim, 1))(expr_x_nondrop_log)
        logits = concatenate(
            [expr_x_drop_log, expr_x_nondrop_log], axis=-1)

        temp_in = Input(shape=(self.in_dim,))
        temp_ = RepeatVector(2)(temp_in)

        temp_ = Permute((2, 1))(temp_)
        samples = Lambda(gumbel_softmax, output_shape=(
            self.in_dim, 2,))([logits, temp_])
        samples = Lambda(lambda x: x[:, :, 1])(samples)
        samples = Reshape(target_shape=(self.in_dim,))(samples)

        out = multiply([expr_x, samples])

        vae = Model(inputs=[expr_in, temp_in], outputs=out)

        loss_func = VAELoss(in_dim, z_log_var, z_mean)

        opt = RMSprop(lr=self.lr)
        vae.compile(optimizer=opt, loss=loss_func)

        ae = Model(inputs=[expr_in, temp_in], outputs=[h1, h2, h3, h2_relu, h3_relu,
                                                       z_mean, z, decoder_h1, decoder_h1_relu,
                                                       decoder_h2, decoder_h2_relu, decoder_h3, decoder_h3_relu,
                                                       samples, out
                                                       ])

        self.vae = vae
        self.ae = ae

        if self.verbose:
            self.vae.summary()

    def fit(self, data):
        &#34;&#34;&#34;
        Fit the given data to the model.

        Parameters
        ----------
        data : array
            Array of training samples where each sample is of size `in_dim`
        &#34;&#34;&#34;
        if self.log:
            data = np.log2(data + 1)
        if self.scale:
            for i in range(data.shape[0]):
                data[i, :] = data[i, :] / np.max(data[i, :])

        tau_in = np.ones(data.shape, dtype=&#39;float32&#39;)

        early_stopping = EarlyStopping(monitor=&#39;loss&#39;, patience=self.patience)

        self.vae.fit([data, tau_in], data, epochs=self.epochs, batch_size=self.batch_size,
                     shuffle=True, verbose=self.verbose, callbacks=[early_stopping])

    def fit_transform(self, data):
        &#34;&#34;&#34;
        Fit the given data to the model and return its transformation

        Parameters
        ----------
        data : array
            Array of training samples where each sample is of size `in_dim`

        Returns
        -------
        array
            Transformed samples, where each sample is of size `out_dim`
        &#34;&#34;&#34;
        self.fit(data)
        return self.transform(data)

    def transform(self, data):
        &#34;&#34;&#34;
        Transform the given data

        Parameters
        ----------
        data : array
            Array of samples to be transformed, where each sample is of size `in_dim`

        Returns
        -------
        array
            Transformed samples, where each sample is of size `out_dim`
        &#34;&#34;&#34;
        return self.ae.predict([data, np.ones(data.shape, dtype=&#39;float32&#39;)])[5]


def sampling(args):
    epsilon_std = 1.0

    if len(args) == 2:
        z_mean, z_log_var = args
        epsilon = K.random_normal(shape=K.shape(z_mean),
                                  mean=0.,
                                  stddev=epsilon_std)

        return z_mean + K.exp(z_log_var / 2) * epsilon
    else:
        z_mean = args[0]
        epsilon = K.random_normal(shape=K.shape(z_mean),
                                  mean=0.,
                                  stddev=epsilon_std)
        return z_mean + K.exp(1.0 / 2) * epsilon


def sampling_gumbel(shape, eps=1e-8):
    u = K.random_uniform(shape)
    return -K.log(-K.log(u+eps)+eps)


def compute_softmax(logits, temp):
    z = logits + sampling_gumbel(K.shape(logits))
    return K.softmax(z / temp)


def gumbel_softmax(args):
    logits, temp = args
    return compute_softmax(logits, temp)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dimdrop.models.vasc.compute_softmax"><code class="name flex">
<span>def <span class="ident">compute_softmax</span></span>(<span>logits, temp)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def compute_softmax(logits, temp):
    z = logits + sampling_gumbel(K.shape(logits))
    return K.softmax(z / temp)</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.gumbel_softmax"><code class="name flex">
<span>def <span class="ident">gumbel_softmax</span></span>(<span>args)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def gumbel_softmax(args):
    logits, temp = args
    return compute_softmax(logits, temp)</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.sampling"><code class="name flex">
<span>def <span class="ident">sampling</span></span>(<span>args)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sampling(args):
    epsilon_std = 1.0

    if len(args) == 2:
        z_mean, z_log_var = args
        epsilon = K.random_normal(shape=K.shape(z_mean),
                                  mean=0.,
                                  stddev=epsilon_std)

        return z_mean + K.exp(z_log_var / 2) * epsilon
    else:
        z_mean = args[0]
        epsilon = K.random_normal(shape=K.shape(z_mean),
                                  mean=0.,
                                  stddev=epsilon_std)
        return z_mean + K.exp(1.0 / 2) * epsilon</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.sampling_gumbel"><code class="name flex">
<span>def <span class="ident">sampling_gumbel</span></span>(<span>shape, eps=1e-08)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sampling_gumbel(shape, eps=1e-8):
    u = K.random_uniform(shape)
    return -K.log(-K.log(u+eps)+eps)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dimdrop.models.vasc.VASC"><code class="flex name class">
<span>class <span class="ident">VASC</span></span>
</code></dt>
<dd>
<section class="desc"><dl>
<dt><strong><a title="dimdrop.models.vasc.VASC" href="#dimdrop.models.vasc.VASC"><code>VASC</code></a></strong> :&ensp;<code>variational</code> <code>autoencoder</code> <code>for</code> <code>scRNA</code>-<code>seq</code> <code>datasets</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class VASC:
    &#34;&#34;&#34;
    VASC: variational autoencoder for scRNA-seq datasets
    &#34;&#34;&#34;

    def __init__(
            self,
            in_dim,
            out_dim,
            epochs=1000,
            patience=3,
            batch_size=100,
            lr=0.01,
            var=False,
            log=True,
            scale=True,
            verbose=0):
        &#34;&#34;&#34;
        Parameters:
        -----------
        in_dim : int
            The input dimension
        out_dim : int
            The output dimension
        epochs: int, optional
            Maximum number of epochs, default `5000`
        patience: int, optional
            The amount of epochs without improvement before the network stops training, default `3`
        batch_size: int, optional
            The batch size for stochastic optimization, default `100`
        lr : float, optional
            The learning rate of the network, default `0.01`
        var: boolean, optional
            Whether to estimate the variance parameters, default `False`
        log: boolean, optional
            Whether log-transformation should be performed, default `True`
        scale: boolean, optional
            Whether scaling (making values within [0,1]) should be performed, default `True`
        &#34;&#34;&#34;
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.epochs = epochs
        self.patience = patience
        self.batch_size = batch_size
        self.lr = lr
        self.var = var
        self.log = log
        self.scale = scale
        self.verbose = verbose
        self.__init_network()

    def __init_network(self):
        in_dim = self.in_dim
        expr_in = Input(shape=(self.in_dim,))

        # The first part of model to recover the expr.
        h0 = Dropout(0.5)(expr_in)
        # Encoder layers
        h1 = Dense(units=512, name=&#39;encoder_1&#39;,
                   kernel_regularizer=regularizers.l1(0.01))(h0)
        h2 = Dense(units=128, name=&#39;encoder_2&#39;)(h1)
        h2_relu = Activation(&#39;relu&#39;)(h2)
        h3 = Dense(units=32, name=&#39;encoder_3&#39;)(h2_relu)
        h3_relu = Activation(&#39;relu&#39;)(h3)

        z_mean = Dense(units=self.out_dim, name=&#39;z_mean&#39;)(h3_relu)
        z_log_var = None
        if self.var:
            z_log_var = Dense(units=2, name=&#39;z_log_var&#39;)(h3_relu)
            z_log_var = Activation(&#39;softplus&#39;)(z_log_var)

        # sampling new samples
            z = Lambda(sampling, output_shape=(
                self.out_dim,))([z_mean, z_log_var])
        else:
            z = Lambda(sampling, output_shape=(self.out_dim,))([z_mean])

        # Decoder layers
        decoder_h1 = Dense(units=32, name=&#39;decoder_1&#39;)(z)
        decoder_h1_relu = Activation(&#39;relu&#39;)(decoder_h1)
        decoder_h2 = Dense(units=128, name=&#39;decoder_2&#39;)(decoder_h1_relu)
        decoder_h2_relu = Activation(&#39;relu&#39;)(decoder_h2)
        decoder_h3 = Dense(units=512, name=&#39;decoder_3&#39;)(decoder_h2_relu)
        decoder_h3_relu = Activation(&#39;relu&#39;)(decoder_h3)
        expr_x = Dense(units=self.in_dim, activation=&#39;sigmoid&#39;)(
            decoder_h3_relu)

        expr_x_drop = Lambda(lambda x: -x ** 2)(expr_x)
        expr_x_drop_p = Lambda(lambda x: K.exp(x))(expr_x_drop)
        expr_x_nondrop_p = Lambda(lambda x: 1-x)(expr_x_drop_p)
        expr_x_nondrop_log = Lambda(lambda x: K.log(x+1e-20))(expr_x_nondrop_p)
        expr_x_drop_log = Lambda(lambda x: K.log(x+1e-20))(expr_x_drop_p)
        expr_x_drop_log = Reshape(target_shape=(
            self.in_dim, 1))(expr_x_drop_log)
        expr_x_nondrop_log = Reshape(target_shape=(
            self.in_dim, 1))(expr_x_nondrop_log)
        logits = concatenate(
            [expr_x_drop_log, expr_x_nondrop_log], axis=-1)

        temp_in = Input(shape=(self.in_dim,))
        temp_ = RepeatVector(2)(temp_in)

        temp_ = Permute((2, 1))(temp_)
        samples = Lambda(gumbel_softmax, output_shape=(
            self.in_dim, 2,))([logits, temp_])
        samples = Lambda(lambda x: x[:, :, 1])(samples)
        samples = Reshape(target_shape=(self.in_dim,))(samples)

        out = multiply([expr_x, samples])

        vae = Model(inputs=[expr_in, temp_in], outputs=out)

        loss_func = VAELoss(in_dim, z_log_var, z_mean)

        opt = RMSprop(lr=self.lr)
        vae.compile(optimizer=opt, loss=loss_func)

        ae = Model(inputs=[expr_in, temp_in], outputs=[h1, h2, h3, h2_relu, h3_relu,
                                                       z_mean, z, decoder_h1, decoder_h1_relu,
                                                       decoder_h2, decoder_h2_relu, decoder_h3, decoder_h3_relu,
                                                       samples, out
                                                       ])

        self.vae = vae
        self.ae = ae

        if self.verbose:
            self.vae.summary()

    def fit(self, data):
        &#34;&#34;&#34;
        Fit the given data to the model.

        Parameters
        ----------
        data : array
            Array of training samples where each sample is of size `in_dim`
        &#34;&#34;&#34;
        if self.log:
            data = np.log2(data + 1)
        if self.scale:
            for i in range(data.shape[0]):
                data[i, :] = data[i, :] / np.max(data[i, :])

        tau_in = np.ones(data.shape, dtype=&#39;float32&#39;)

        early_stopping = EarlyStopping(monitor=&#39;loss&#39;, patience=self.patience)

        self.vae.fit([data, tau_in], data, epochs=self.epochs, batch_size=self.batch_size,
                     shuffle=True, verbose=self.verbose, callbacks=[early_stopping])

    def fit_transform(self, data):
        &#34;&#34;&#34;
        Fit the given data to the model and return its transformation

        Parameters
        ----------
        data : array
            Array of training samples where each sample is of size `in_dim`

        Returns
        -------
        array
            Transformed samples, where each sample is of size `out_dim`
        &#34;&#34;&#34;
        self.fit(data)
        return self.transform(data)

    def transform(self, data):
        &#34;&#34;&#34;
        Transform the given data

        Parameters
        ----------
        data : array
            Array of samples to be transformed, where each sample is of size `in_dim`

        Returns
        -------
        array
            Transformed samples, where each sample is of size `out_dim`
        &#34;&#34;&#34;
        return self.ae.predict([data, np.ones(data.shape, dtype=&#39;float32&#39;)])[5]</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dimdrop.models.vasc.VASC.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, in_dim, out_dim, epochs=1000, patience=3, batch_size=100, lr=0.01, var=False, log=True, scale=True, verbose=0)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters:</h2>
<dl>
<dt><strong><code>in_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The input dimension</dd>
<dt><strong><code>out_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The output dimension</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of epochs, default <code>5000</code></dd>
<dt><strong><code>patience</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The amount of epochs without improvement before the network stops training, default <code>3</code></dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The batch size for stochastic optimization, default <code>100</code></dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The learning rate of the network, default <code>0.01</code></dd>
<dt><strong><code>var</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether to estimate the variance parameters, default <code>False</code></dd>
<dt><strong><code>log</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether log-transformation should be performed, default <code>True</code></dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>boolean</code>, optional</dt>
<dd>Whether scaling (making values within [0,1]) should be performed, default <code>True</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(
        self,
        in_dim,
        out_dim,
        epochs=1000,
        patience=3,
        batch_size=100,
        lr=0.01,
        var=False,
        log=True,
        scale=True,
        verbose=0):
    &#34;&#34;&#34;
    Parameters:
    -----------
    in_dim : int
        The input dimension
    out_dim : int
        The output dimension
    epochs: int, optional
        Maximum number of epochs, default `5000`
    patience: int, optional
        The amount of epochs without improvement before the network stops training, default `3`
    batch_size: int, optional
        The batch size for stochastic optimization, default `100`
    lr : float, optional
        The learning rate of the network, default `0.01`
    var: boolean, optional
        Whether to estimate the variance parameters, default `False`
    log: boolean, optional
        Whether log-transformation should be performed, default `True`
    scale: boolean, optional
        Whether scaling (making values within [0,1]) should be performed, default `True`
    &#34;&#34;&#34;
    self.in_dim = in_dim
    self.out_dim = out_dim
    self.epochs = epochs
    self.patience = patience
    self.batch_size = batch_size
    self.lr = lr
    self.var = var
    self.log = log
    self.scale = scale
    self.verbose = verbose
    self.__init_network()</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.VASC.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit the given data to the model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>array</code></dt>
<dd>Array of training samples where each sample is of size <code>in_dim</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit(self, data):
    &#34;&#34;&#34;
    Fit the given data to the model.

    Parameters
    ----------
    data : array
        Array of training samples where each sample is of size `in_dim`
    &#34;&#34;&#34;
    if self.log:
        data = np.log2(data + 1)
    if self.scale:
        for i in range(data.shape[0]):
            data[i, :] = data[i, :] / np.max(data[i, :])

    tau_in = np.ones(data.shape, dtype=&#39;float32&#39;)

    early_stopping = EarlyStopping(monitor=&#39;loss&#39;, patience=self.patience)

    self.vae.fit([data, tau_in], data, epochs=self.epochs, batch_size=self.batch_size,
                 shuffle=True, verbose=self.verbose, callbacks=[early_stopping])</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.VASC.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit the given data to the model and return its transformation</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>array</code></dt>
<dd>Array of training samples where each sample is of size <code>in_dim</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>array</code></strong></dt>
<dd>Transformed samples, where each sample is of size <code>out_dim</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_transform(self, data):
    &#34;&#34;&#34;
    Fit the given data to the model and return its transformation

    Parameters
    ----------
    data : array
        Array of training samples where each sample is of size `in_dim`

    Returns
    -------
    array
        Transformed samples, where each sample is of size `out_dim`
    &#34;&#34;&#34;
    self.fit(data)
    return self.transform(data)</code></pre>
</details>
</dd>
<dt id="dimdrop.models.vasc.VASC.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<section class="desc"><p>Transform the given data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>array</code></dt>
<dd>Array of samples to be transformed, where each sample is of size <code>in_dim</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>array</code></strong></dt>
<dd>Transformed samples, where each sample is of size <code>out_dim</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def transform(self, data):
    &#34;&#34;&#34;
    Transform the given data

    Parameters
    ----------
    data : array
        Array of samples to be transformed, where each sample is of size `in_dim`

    Returns
    -------
    array
        Transformed samples, where each sample is of size `out_dim`
    &#34;&#34;&#34;
    return self.ae.predict([data, np.ones(data.shape, dtype=&#39;float32&#39;)])[5]</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dimdrop.models" href="index.html">dimdrop.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dimdrop.models.vasc.compute_softmax" href="#dimdrop.models.vasc.compute_softmax">compute_softmax</a></code></li>
<li><code><a title="dimdrop.models.vasc.gumbel_softmax" href="#dimdrop.models.vasc.gumbel_softmax">gumbel_softmax</a></code></li>
<li><code><a title="dimdrop.models.vasc.sampling" href="#dimdrop.models.vasc.sampling">sampling</a></code></li>
<li><code><a title="dimdrop.models.vasc.sampling_gumbel" href="#dimdrop.models.vasc.sampling_gumbel">sampling_gumbel</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dimdrop.models.vasc.VASC" href="#dimdrop.models.vasc.VASC">VASC</a></code></h4>
<ul class="">
<li><code><a title="dimdrop.models.vasc.VASC.__init__" href="#dimdrop.models.vasc.VASC.__init__">__init__</a></code></li>
<li><code><a title="dimdrop.models.vasc.VASC.fit" href="#dimdrop.models.vasc.VASC.fit">fit</a></code></li>
<li><code><a title="dimdrop.models.vasc.VASC.fit_transform" href="#dimdrop.models.vasc.VASC.fit_transform">fit_transform</a></code></li>
<li><code><a title="dimdrop.models.vasc.VASC.transform" href="#dimdrop.models.vasc.VASC.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>